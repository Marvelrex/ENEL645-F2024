{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection, Overfitting and Regularization\n",
    "\n",
    "This tutorial is meant to be a gentle introduction to machine learning concepts. We present a simple polynomial fitting example using a least squares solution, which is a specific case of what is called maximum likelihood, but we will not get into details about this probabilistic view of least squares in this tutorial. We use this example to introduce important machine learning concepts using plain language that should be accessible to undergradiuate and graduate students with a minimum background of calculus.\n",
    "\n",
    "The goals of this tutorial are:\n",
    "  - Explain how to develop an experiment. Split your data into development set (*i.e.*, train and validaion sets) and test set.   \n",
    "  - Introduce how to select your model.\n",
    "  - Introduce the concepts of *over-fitting*, *under-fitting*, and *model generalization*.\n",
    "  - Introduce the concept of *regularization* for reducing *over-fitting*.\n",
    " \n",
    "\n",
    "This tutorial is interactive and it corresponds to an adaptation of the example presented in chapter 1 of the book: **Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA.**\n",
    "\n",
    "## Designing your experiment\n",
    "\n",
    "Machine learning builds models by learning from data. When designing your experiment, you need to split your data into a development set and a test set. The development set is split into 2 sets: a train set and a validation set. The train set is used to learn the parameters of the different models you are fititng (training). The validation set is employed to select hopefully what is the best model among the different models you trained, therefore it has a bias and cannot be used as proof of generalization. The test set is used to see if the selected model generalizes well to unseen data. \n",
    "\n",
    "<img src=\"../Figures/train_val_test.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "## Generating synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directive to make plots inline as opposed to having pop-up plots \n",
    "%matplotlib inline \n",
    "import numpy as np # Import numpy with nickname np\n",
    "import matplotlib.pylab as plt # plotting library\n",
    "from ipywidgets import * # Interaction library\n",
    "\n",
    "var = 0.2 #Noise variance\n",
    "\n",
    "#Create data set\n",
    "N = 25\n",
    "x = np.linspace(0, 1, N) \n",
    "y_noiseless = np.sin(2*np.pi*x) # signal\n",
    "y = y_noiseless + np.random.normal(0, var, N) #signal + noise -> real measurements always come with noise\n",
    "\n",
    "\n",
    "# Plot entire data set with and without noise\n",
    "plt.figure()\n",
    "plt.plot(x,y_noiseless,linewidth = 2.0,label = r'Data without noise: $sin(2 \\pi x)$')\n",
    "plt.scatter(x,y,color ='red', marker = 'x', label = r'Data with noise')\n",
    "plt.legend(loc = (0.02, 0.18))\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data in train/validation/test sets - size of each set was choosen arbitrarily\n",
    "train_size = 10\n",
    "val_size = 5\n",
    "test_size = 10\n",
    "\n",
    "indexes = np.arange(N, dtype =int)\n",
    "np.random.seed(seed = 2) # Random seed to keep results always the same\n",
    "np.random.shuffle(indexes) # Shuffling the data before the split\n",
    "\n",
    "# Train set\n",
    "aux = indexes[:train_size]\n",
    "aux = np.sort(aux)\n",
    "x_train = x[aux]\n",
    "y_train = y[aux] \n",
    "\n",
    "#Validation set\n",
    "aux = indexes[train_size: train_size + val_size]\n",
    "aux = np.sort(aux)\n",
    "x_val= x[aux]\n",
    "y_val = y[aux]\n",
    "\n",
    "# Test set\n",
    "aux = indexes[-test_size:]\n",
    "aux = np.sort(aux)\n",
    "x_test = x[aux]\n",
    "y_test = y[aux]\n",
    "\n",
    "# Plot train/val/test sets\n",
    "plt.figure()\n",
    "plt.plot(x,y_noiseless,linewidth = 2.0,label = r'Model no noise: $sin(2 \\pi x)$')\n",
    "plt.scatter(x_train,y_train,color ='red', marker = 'x', label = \"Train set\")\n",
    "plt.scatter(x_val,y_val,color = 'blue',marker = '^' , label = \"Validation set\")\n",
    "plt.scatter(x_test,y_test,color = 'green', marker = 'o', label = \"Test set\")\n",
    "plt.legend(loc = (0.02, 0.18))\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Observations: $$\\boldsymbol{X} =[x_1,x_2,...,x_N]^T$$\n",
    "Target:  $$\\boldsymbol{T} =[t_1,t_2,...,t_N]^T$$\n",
    "Estimates:  $$\\boldsymbol{Y} =[y_1,y_2,...,y_N]^T$$\n",
    "\n",
    "\n",
    "## Polynomial Model\n",
    "\n",
    "$$y(x,\\boldsymbol{W})= w_0 + w_1x +w_2x^2+...+w_mx^m = \\sum^M_{j=0}w_jx^j$$\n",
    "\n",
    "Weights (*i.e.*, what our model learns):  $$\\boldsymbol{W} =[w_1,w_2,...,w_M]^T$$\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "Quadratic cost function: $$E(\\boldsymbol{W})=\\frac{1}{2}\\sum_{n=1}^N\\{y(x_n,\\boldsymbol{W})-t_n\\}^2$$\n",
    "\n",
    "Computing the derivative of the cost function and making it equal to zero, we can find the vector **W*** that minimizes the error:\n",
    "$$ \\boldsymbol{W}^* = (\\boldsymbol{A}^T\\boldsymbol{A})^{-1}\\boldsymbol{A} ^T\\boldsymbol{T}$$\n",
    "\n",
    "Where **A** is defined by:\n",
    "\n",
    "$$\\boldsymbol{A} = \\begin{bmatrix}\n",
    "    1 & x_{1} & x_{1}^2 & \\dots  & x_{1}^M \\\\\n",
    "    1 & x_{2} & x_{2}^2 & \\dots  & x_{2}^M \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    1 & x_{N} & x_{N}^2 & \\dots  & x_{N}^M\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Least squares polynomial fitting solution\n",
    "# Implementation of the equation shown above \n",
    "def polynomial_fit(X,T,M):\n",
    "    A = np.power(X.reshape(-1,1),np.arange(0,M+1).reshape(1,-1))\n",
    "    T = T.reshape(-1,1)\n",
    "    W = np.dot(np.linalg.pinv(A),T)\n",
    "    return W.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the least squares result varying the polynomial degree between  0 a 9. **Which model is a good model?** Look at the plots but also the magnitude of the weights resulting from each polynomial fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotmodel(M):\n",
    "    coefs = polynomial_fit(x_train, y_train, M)[::-1]\n",
    "    print(\"Weights:\\n\", coefs)\n",
    "    p = np.poly1d(coefs)\n",
    "    plt.figure()\n",
    "    plt.plot(x,y_noiseless,linewidth = 1.5,label = r'Data no noise: $sin(2 \\pi x)$')\n",
    "    plt.scatter(x_train,y_train,color='red',label= \"Train set\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(r'y')\n",
    "    y_fit = p(x_train) \n",
    "    plt.plot(x_train,y_fit,linewidth = 1.0,label =\"Polynomial Fit\")\n",
    "    plt.legend(loc=(0.02,0.02))\n",
    "    plt.show()\n",
    "\n",
    "interact(plotmodel,M=(0,9,1))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the degree, M, of the polynomial we fit to our data, our model falls under one of these categories:\n",
    "\n",
    "- **Under-fitting**: the model is too inflexible and is not able to capture any patterns in the data.\n",
    "\n",
    "- **Over-fitting**: the model is too flexible. It ends up tuning to the random noise in the data. The model may have a low error in the train set, but it is not expected to generalize well to new (unseen) data.\n",
    "\n",
    "- **Good fit**: The model is able to capture patterns in our data, but it does not get tuned to the random noise in the data. Better chances to generalize to new  (unseen) data.\n",
    "\n",
    "A good exercise is to visually determine whether the model is under-fitting, over-fitting or it is a good model based on the polynomial degree in the interactive plot shown above. \n",
    "\n",
    "## Root mean squared error and Model Selection\n",
    "\n",
    "Root mean squared error is an error measure commonly emplyed in regression problems.\n",
    "\n",
    "$$E_{RMS}=\\sqrt{2E(\\boldsymbol{W^*})/N}$$\n",
    "\n",
    "We will analyze the root mean squared error in the validation set to select our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes RMS error\n",
    "def rms_error(X,T,W):\n",
    "    p = np.poly1d(W)\n",
    "    T_fit = p(X)\n",
    "    E = np.sqrt(((T - T_fit)**2/T.size).sum())\n",
    "    return E\n",
    "   \n",
    "m = range(10)\n",
    "train = []\n",
    "val = []\n",
    "\n",
    "# Compute RMS error across different polynomial fits\n",
    "for M in m:\n",
    "    W = polynomial_fit(x_train, y_train, M)[::-1]\n",
    "    error_train = rms_error(x_train,y_train,W)\n",
    "    error_val = rms_error(x_val,y_val,W)\n",
    "    train.append(error_train)\n",
    "    val.append(error_val)\n",
    "\n",
    "# Plot the errors\n",
    "plt.figure()\n",
    "plt.plot(m,train,linewidth = 2.0,marker = 'o',markersize = 12,label = r'$E_{RMS}$ Train')\n",
    "plt.plot(m,val,linewidth = 2.0,marker = 'x',markersize = 12,label = r'$E_{RMS}$ Validation')\n",
    "plt.legend(loc = (0.02, 0.05))\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.ylabel(r'$E_{RMS}$')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Model selection - the model with the lowest error in the validation set is selected. Then, the model \n",
    "# generalizability is assessed on the test set. \n",
    "best_M = np.argmin(val)\n",
    "W = polynomial_fit(x_train, y_train, best_M)[::-1]\n",
    "test_error = rms_error(x_test,y_test,W)\n",
    "print(\"Model selected was a  polynomial of degree %d\" %best_M)\n",
    "print(\"Root mean squared test error: %.3f\" %test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function with regularization\n",
    "\n",
    "Regularization is a technique to avoid overfitting. Do you remember how the values of the estimated weights increased quickly for polynomial fits with high degrees in the example without regularization? That was the model tuning itself to the noise in the data. Regularization consists in adding a penalty term to the cost function. Let's add a quadratic penalty to the weights we are trying to estimate. The quadratic penalty is called **L2 regularization**. \n",
    "\n",
    "$$E(\\boldsymbol{W})=\\frac{1}{2}\\sum_{n=1}^N\\{y(x_n,\\boldsymbol{W})-t_n\\}^2 +\\frac{\\lambda}{2}||\\boldsymbol{W}||^2$$\n",
    "\n",
    "The above equation also has a well-defined minimum point. Computing its derivative and making it equal to zero, the solution of the equation is given by:\n",
    "\n",
    "$$\\boldsymbol{W}^* = (\\boldsymbol{A}^T\\boldsymbol{A} + \\lambda n\\boldsymbol{I})^{-1}\\boldsymbol{A} ^T\\boldsymbol{T} $$\n",
    "\n",
    "Note that our problem now has two hyper-parameters that we need to set. The polynomial degree (M) and the regularization factor ($\\lambda$). Hyper-parameters are set by the user (*e.g.*, M and $\\lambda$), while parameters are learned by the model (*e.g.*, the weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Least square solution with regularization\n",
    "def polynomial_fit_reg(X,T,M,lamb):\n",
    "    N = X.shape[0]\n",
    "    A = np.power(X.reshape(-1,1),np.arange(0,M+1).reshape(1,-1))\n",
    "    lambda_matrix = lamb*N*np.eye(M+1)\n",
    "    T = T.reshape(-1,1)\n",
    "    aux = np.dot(A.T,A) + lambda_matrix\n",
    "    aux = np.linalg.pinv(aux)\n",
    "    aux2 = np.dot(A.T,T)\n",
    "    W = np.dot(aux,aux2)\n",
    "    return W.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the demo below, we show the influence of $log(\\lambda)$ and $M$ in the polynomial fitting. Note the influence of $\\lambda$  in the estimated weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotmodel2(M,log_lamb):\n",
    "    lamb = np.exp(log_lamb)\n",
    "    coefs = polynomial_fit_reg(x_train, y_train, M,lamb)[::-1]\n",
    "    print(\"Weights:\\n\",coefs)\n",
    "    print(\"Lambda\\n\", lamb)\n",
    "    p = np.poly1d(coefs)\n",
    "    plt.figure()\n",
    "    plt.plot(x,y_noiseless,linewidth = 1.5,label = r'Data no noise: $sin(2 \\pi x)$')\n",
    "    plt.scatter(x_train,y_train,color='red',label= \"Train set\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(r'y')\n",
    "    y_fit = p(x_train) \n",
    "    plt.plot(x_train,y_fit,linewidth = 1.0,label =\"Polynomial Fit\")\n",
    "    plt.legend(loc=(0.02,0.02))\n",
    "    plt.show()\n",
    "interact(plotmodel2,M=(0,9,1),log_lamb = (-40,-9,.1))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we fit our model to the training data, we do a grid search through different polynomial degrees (M) and different regularization values ($\\lambda$) to search for the model with lowest error in the validation set, which again is the model we select. An alternative to the grid search is to perform a random search for the best set of model hyper-maraters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lamb = range(-40,-8) # regularization values\n",
    "M = range(7,10) # different polynomial degrees\n",
    "train = np.zeros((len(log_lamb), len(M)))\n",
    "val = np.zeros((len(log_lamb), len(M)))\n",
    "\n",
    "for (i,m) in enumerate(M):\n",
    "    for (j,l) in enumerate(log_lamb):\n",
    "        lamb = np.exp(l)\n",
    "        coeffs = polynomial_fit_reg(x_train, y_train, m,lamb)[::-1]\n",
    "        train[j,i] = rms_error(x_train,y_train,coeffs)\n",
    "        val[j,i] = rms_error(x_val,y_val,coeffs)\n",
    "    \n",
    "plt.figure(figsize = (24,22), dpi = 300)\n",
    "for (i,m) in enumerate(M):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.plot(log_lamb,train[:,i],linewidth = 1.0,marker = 'o',markersize = 12,label = r'$E_{RMS}$ Train')\n",
    "    plt.plot(log_lamb,val[:,i],linewidth = 1.0,marker = 'x',markersize = 12,label = r'$E_{RMS}$ Validation')\n",
    "    plt.legend(loc = (0.02, 0.075))\n",
    "    plt.xlabel(r'$ln\\lambda$')\n",
    "    plt.ylabel(r'$E_{RMS}$')\n",
    "    plt.title(\"Polynomial degree %d\" %m)\n",
    "plt.show()\n",
    "\n",
    "# Model selection\n",
    "best_M_reg = np.unravel_index(val.argmin(), val.shape)\n",
    "W = polynomial_fit_reg(x_train, y_train, M[best_M_reg[1]], np.exp(log_lamb[best_M_reg[0]]))[::-1]\n",
    "test_error = rms_error(x_test,y_test,W)\n",
    "print(\"Model selected was a  polynome of degree %d with lambda = %e\" %(M[best_M_reg[1]], np.exp(log_lamb[best_M_reg[0]])))\n",
    "print(\"Root mean squared test error: %.3f\" %test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "That is all folks. In this tutorial, we presented a gentle introduction to model selection, over-fitting and regularization. We saw how to design our experiment by splitting our dataset into a development set (train + validation sets) and a test set. This method is commonly employed when we have very large datasets that may take days to train. For smaller datasets, a procedure called [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#:~:text=Cross%2Dvalidation%2C%20sometimes%20called%20rotation,to%20an%20independent%20data%20set.) is often employed. We also saw that polynomials with high degrees tended to overfit to the data and by adding a regularization term to the cost function, over-fitting can be potentially mitigated. Another way to avoid over-fitting is by collecting more data (see activity suggestions), which is not always feasible. \n",
    "\n",
    "The concepts explained in this tutorial are valid not just for polynomial fits, but also across diffrent machine learning models like neural networks and support vector machines.\n",
    "\n",
    "\n",
    "\n",
    "## Activity suggestions\n",
    "\n",
    "- Use more data for training your model;\n",
    "- Change the input signal;\n",
    "- Change the noise intensity;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "249d11310564531dbb0422c65726fbafe5d71a3f15733fe196d56460bed7c227"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
